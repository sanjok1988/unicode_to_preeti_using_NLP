{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "97891465-81b3-4994-9e20-3d11e549522b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion complete. Output saved to phrase_dataset_unicode_to_preeti.csv\n"
     ]
    }
   ],
   "source": [
    "# Dictionary mapping Unicode Nepali characters to Preeti font characters\n",
    "unicodeToPreetiDict = {\n",
    "    \"अ\": \"c\", \"आ\": \"cf\", \"ा\": \"f\", \"इ\": \"O\", \"ई\": \"O{\", \"र्\": \"{\",\n",
    "    \"उ\": \"p\", \"ए\": \"P\", \"े\": \"]\", \"ै\": \"}\", \"ो\": \"f]\", \"ौ\": \"f}\",\n",
    "    \"ओ\": \"cf]\", \"औ\": \"cf}\", \"ं\": \"+\", \"ँ\": \"F\", \"ि\": \"l\", \"ी\": \"L\",\n",
    "    \"ु\": \"'\", \"ू\": '\"', \"क\": \"s\", \"ख\": \"v\", \"ग\": \"u\", \"घ\": \"3\",\n",
    "    \"ङ\": \"ª\", \"च\": \"r\", \"छ\": \"5\", \"ज\": \"h\", \"झ\": \"´\", \"ञ\": \"`\",\n",
    "    \"ट\": \"6\", \"ठ\": \"7\", \"ड\": \"8\", \"ढ\": \"9\", \"ण\": \"0f\", \"त\": \"t\",\n",
    "    \"थ\": \"y\", \"द\": \"b\", \"ध\": \"w\", \"न\": \"g\", \"प\": \"k\", \"फ\": \"km\",\n",
    "    \"ब\": \"a\", \"भ\": \"e\", \"म\": \"d\", \"य\": \"o\", \"र\": \"/\", \"रू\": \"?\",\n",
    "    \"ृ\": \"[\", \"ल\": \"n\", \"व\": \"j\", \"स\": \";\", \"श\": \"z\", \"ष\": \"if\",\n",
    "    \"ज्ञ\": \"1\", \"ह\": \"x\", \"१\": \"!\", \"२\": \"@\", \"३\": \"#\", \"४\": \"$\",\n",
    "    \"५\": \"%\", \"६\": \"^\", \"७\": \"&\", \"८\": \"*\", \"९\": \"(\", \"०\": \")\",\n",
    "    \"।\": \".\", \"्\": \"\\\\\", \"ऊ\": \"pm\", \"-\": \" \", \"(\": \"-\", \")\": \"_\"\n",
    "}\n",
    "\n",
    "def normalize_unicode(unicode_text):\n",
    "    \"\"\"\n",
    "    Normalizes unicode text to handle special cases in Nepali text\n",
    "    before converting to Preeti.\n",
    "    \"\"\"\n",
    "    index = -1\n",
    "    normalized = ''\n",
    "    \n",
    "    while index + 1 < len(unicode_text):\n",
    "        index += 1\n",
    "        character = unicode_text[index]\n",
    "        \n",
    "        try:\n",
    "            try:\n",
    "                if character != 'र':\n",
    "                    if (index + 2 < len(unicode_text) and \n",
    "                        unicode_text[index+1] == '्' and \n",
    "                        unicode_text[index+2] not in [' ', '।', ',']):\n",
    "                        \n",
    "                        if unicode_text[index+2] != 'र':\n",
    "                            if unicodeToPreetiDict[character] in list('wertyuxasdghjkzvn'):\n",
    "                                normalized += chr(ord(unicodeToPreetiDict[character]) - 32)\n",
    "                                index += 1\n",
    "                                continue\n",
    "                            elif character == 'स':\n",
    "                                normalized += ':'\n",
    "                                index += 1\n",
    "                                continue\n",
    "                            elif character == 'ष':\n",
    "                                normalized += 'i'\n",
    "                                index += 1\n",
    "                                continue\n",
    "                                \n",
    "                if (index > 0 and unicode_text[index-1] != 'र' and \n",
    "                    character == '्' and \n",
    "                    index + 1 < len(unicode_text) and \n",
    "                    unicode_text[index+1] == 'र'):\n",
    "                    \n",
    "                    if unicode_text[index-1] not in ['ट', 'ठ', 'ड']:\n",
    "                        normalized += '|'  # for sign as in क्रम\n",
    "                    else:\n",
    "                        normalized += '«'  # for sign as in ट्रक\n",
    "                    index += 1\n",
    "                    continue\n",
    "                    \n",
    "            except IndexError:\n",
    "                pass\n",
    "            \n",
    "            normalized += character\n",
    "            \n",
    "        except KeyError:\n",
    "            normalized += character\n",
    "    \n",
    "    # Replace specific combinations\n",
    "    normalized = normalized.replace('त|', 'q')  # for त्र\n",
    "    return normalized\n",
    "\n",
    "def convert_to_preeti(normalized_unicode):\n",
    "    \"\"\"\n",
    "    Converts normalized unicode text to Preeti font format.\n",
    "    \"\"\"\n",
    "    converted = ''\n",
    "    index = -1\n",
    "    \n",
    "    while index + 1 < len(normalized_unicode):\n",
    "        index += 1\n",
    "        character = normalized_unicode[index]\n",
    "        \n",
    "        # Skip BOM character if present\n",
    "        if character == '\\ufeff':\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            try:\n",
    "                # Handle special cases for 'ि' (hraswo ukaar)\n",
    "                if index + 1 < len(normalized_unicode) and normalized_unicode[index+1] == 'ि':\n",
    "                    if character == 'q':\n",
    "                        converted += 'l' + character\n",
    "                    else:\n",
    "                        converted += 'l' + unicodeToPreetiDict[character]\n",
    "                    index += 1\n",
    "                    continue\n",
    "                \n",
    "                # Handle constructs like त्ति\n",
    "                if (index + 2 < len(normalized_unicode) and \n",
    "                    normalized_unicode[index+2] == 'ि'):\n",
    "                    if character in list('WERTYUXASDGHJK:ZVN'):\n",
    "                        if normalized_unicode[index+1] != 'q':\n",
    "                            converted += 'l' + character + unicodeToPreetiDict[normalized_unicode[index+1]]\n",
    "                            index += 2\n",
    "                            continue\n",
    "                        else:\n",
    "                            converted += 'l' + character + normalized_unicode[index+1]\n",
    "                            index += 2\n",
    "                            continue\n",
    "                \n",
    "                # Handle reph as in वार्ता\n",
    "                if (index + 1 < len(normalized_unicode) and \n",
    "                    normalized_unicode[index+1] == '्' and \n",
    "                    character == 'र'):\n",
    "                    \n",
    "                    if (index + 3 < len(normalized_unicode) and \n",
    "                        normalized_unicode[index+3] in ['ा', 'ो', 'ौ', 'े', 'ै', 'ी']):\n",
    "                        \n",
    "                        converted += (unicodeToPreetiDict[normalized_unicode[index+2]] + \n",
    "                                      unicodeToPreetiDict[normalized_unicode[index+3]] + '{')\n",
    "                        index += 3\n",
    "                        continue\n",
    "                        \n",
    "                    elif index + 3 < len(normalized_unicode) and normalized_unicode[index+3] == 'ि':\n",
    "                        converted += (unicodeToPreetiDict[normalized_unicode[index+3]] + \n",
    "                                      unicodeToPreetiDict[normalized_unicode[index+2]] + '{')\n",
    "                        index += 3\n",
    "                        continue\n",
    "                        \n",
    "                    converted += unicodeToPreetiDict[normalized_unicode[index+2]] + '{'\n",
    "                    index += 2\n",
    "                    continue\n",
    "                \n",
    "                # Handle the likes of ष्ट्रिय\n",
    "                if (index + 3 < len(normalized_unicode) and \n",
    "                    normalized_unicode[index+3] == 'ि' and \n",
    "                    (normalized_unicode[index+2] == '|' or normalized_unicode[index+2] == '«')):\n",
    "                    \n",
    "                    if character in list('WERTYUXASDGHJK:ZVNIi'):\n",
    "                        converted += ('l' + character + \n",
    "                                      unicodeToPreetiDict[normalized_unicode[index+1]] + \n",
    "                                      normalized_unicode[index+2])\n",
    "                        index += 3\n",
    "                        continue\n",
    "                \n",
    "            except IndexError:\n",
    "                pass\n",
    "            \n",
    "            converted += unicodeToPreetiDict[character]\n",
    "            \n",
    "        except KeyError:\n",
    "            converted += character\n",
    "    \n",
    "    # Replace specific combinations with their Preeti equivalents\n",
    "    replacements = [\n",
    "        ('Si', 'I'),        # Si in preeti is aadha ka aadha ष, so replace with I which is aadha क्ष\n",
    "        ('H`', '1'),        # H` is the product of composite nature of unicode ज्ञ\n",
    "        ('b\\\\w', '4'),      # b\\w means in preeti द halanta ध, so replace the composite\n",
    "        ('z|', '>'),        # composite for श्र\n",
    "        (\"/'\", '?'),        # composite for रु\n",
    "        ('/\"', '¿'),        # composite for रू\n",
    "        ('Tt', 'Q'),        # composite for त्त\n",
    "        ('b\\\\lj', 'lå'),    # composite for द्वि\n",
    "        ('b\\\\j', 'å'),      # composite for द्व\n",
    "        ('0f\\\\\\\\', '0'),    # composite for ण् to get the aadha ण in say गण्डक\n",
    "        ('`\\\\\\\\', '~')      # composite for aadha ञ्\n",
    "    ]\n",
    "    \n",
    "    for old, new in replacements:\n",
    "        converted = converted.replace(old, new)\n",
    "        \n",
    "    return converted\n",
    "\n",
    "def convert_unicode_to_preeti(input_file, output_file):\n",
    "    \"\"\"\n",
    "    Read Unicode Nepali words from input CSV and convert to Preeti\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Read the input CSV file\n",
    "    df = pd.read_csv(input_file, encoding='utf-8')\n",
    "    \n",
    "    # Prepare output data\n",
    "    output_data = []\n",
    "    \n",
    "    # Convert each word\n",
    "    for word in df['unicode']:\n",
    "        # Normalize and convert the word\n",
    "        normalized_word = normalize_unicode(word)\n",
    "        preeti_word = convert_to_preeti(normalized_word)\n",
    "        \n",
    "        # Add to output data\n",
    "        output_data.append([word, preeti_word])\n",
    "    \n",
    "    # Create output DataFrame\n",
    "    output_df = pd.DataFrame(output_data, columns=['Unicode Word', 'Preeti Conversion'])\n",
    "    \n",
    "    # Save to CSV\n",
    "    output_df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "    \n",
    "    print(f\"Conversion complete. Output saved to {output_file}\")\n",
    "\n",
    "# Example usage\n",
    "input_file = 'output_short_phrases.csv'\n",
    "output_file = 'phrase_dataset_unicode_to_preeti.csv'\n",
    "\n",
    "# Ensure your input CSV has a column named 'Word'\n",
    "# Example input CSV content:\n",
    "# Word\n",
    "# नमस्ते\n",
    "# धन्यवाद\n",
    "# स्वागत\n",
    "# माफ गर्नुहोस्\n",
    "# कृपया\n",
    "\n",
    "convert_unicode_to_preeti(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "54258719-a074-4632-a7d2-255185cf3c6a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Word\n",
      "0    अ\n",
      "1    आ\n",
      "2    इ\n",
      "3    ई\n",
      "4    उ\n",
      "Total elements (rows × columns): 761029\n",
      "Total number of rows: 761029\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "input_csv = 'output_words.csv'\n",
    "df = pd.read_csv(input_csv, encoding='utf-8')\n",
    "\n",
    "# Display the first few rows\n",
    "print(df.head())\n",
    "\n",
    "# Get the total number of elements (rows × columns)\n",
    "print(\"Total elements (rows × columns):\", df.size)\n",
    "\n",
    "# If you want number of rows instead, use:\n",
    "print(\"Total number of rows:\", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cc4720cf-cc76-4b5c-8296-656e5d8bd647",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 20000 words to 'unicode_words_5000.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def extract_words(input_file, input_column, output_file, max_words=20000):\n",
    "    df = pd.read_csv(input_file, encoding='utf-8')\n",
    "    \n",
    "    # Flatten all words from the sentences\n",
    "    all_words = []\n",
    "    for sentence in df[input_column].dropna():\n",
    "        all_words.extend(sentence.split())\n",
    "        if len(all_words) >= max_words:\n",
    "            break\n",
    "    \n",
    "    # Trim to exactly 5000\n",
    "    all_words = all_words[:max_words]\n",
    "    \n",
    "    # Create DataFrame\n",
    "    words_df = pd.DataFrame(all_words, columns=['unicode'])\n",
    "    \n",
    "    # Save to CSV\n",
    "    words_df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "    print(f\"Saved {len(words_df)} words to '{output_file}'\")\n",
    "\n",
    "# Example usage\n",
    "extract_words('unicode_preeti_dataset.csv', 'unicode', 'unicode_words_5000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7cc91ae0-6ceb-4eb2-89e3-fbfeca54b757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('अ', 'c'), ('आ', 'cf'), ('इ', 'O'), ('ई', 'O{'), ('उ', 'p')]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the input CSV (only one column: 'unicode')\n",
    "df = pd.read_csv('unicode_words_5000.csv', encoding='utf-8')\n",
    "\n",
    "# Drop NaN and convert to list\n",
    "words = df['unicode'].dropna().tolist()\n",
    "\n",
    "# Convert using your function\n",
    "converted_words = convert_word_list(words)\n",
    "\n",
    "\n",
    "print(converted_words[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7bb59059-d0f0-434c-89a9-648d5ef1214e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved converted words to 'converted_preeti_words.csv'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Save to new CSV\n",
    "output_df = pd.DataFrame({'preeti': converted_words})\n",
    "output_df.to_csv('converted_preeti_words.csv', index=False, encoding='utf-8')\n",
    "\n",
    "print(\"Saved converted words to 'converted_preeti_words.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "084c000c-be5e-44e7-b1c2-4646cbbc258f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the path to the folder containing .txt files:  /Users/sanjokdangol/Downloads/archive/SuchanaPrabidhi/SuchanaPrabidhi\n",
      "Enter the path to save the output CSV file:  infotech.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: 289.txt\n",
      "Processed: 504.txt\n",
      "Processed: 262.txt\n",
      "Processed: 276.txt\n",
      "Processed: 510.txt\n",
      "Processed: 538.txt\n",
      "Processed: 909.txt\n",
      "Processed: 921.txt\n",
      "Processed: 935.txt\n",
      "Processed: 706.txt\n",
      "Processed: 712.txt\n",
      "Processed: 29.txt\n",
      "Processed: 869.txt\n",
      "Processed: 15.txt\n",
      "Processed: 855.txt\n",
      "Processed: 699.txt\n",
      "Processed: 841.txt\n",
      "Processed: 114.txt\n",
      "Processed: 672.txt\n",
      "Processed: 666.txt\n",
      "Processed: 100.txt\n",
      "Processed: 128.txt\n",
      "Processed: 896.txt\n",
      "Processed: 882.txt\n",
      "Processed: 470.txt\n",
      "Processed: 316.txt\n",
      "Processed: 302.txt\n",
      "Processed: 464.txt\n",
      "Processed: 458.txt\n",
      "Processed: 459.txt\n",
      "Processed: 303.txt\n",
      "Processed: 465.txt\n",
      "Processed: 471.txt\n",
      "Processed: 317.txt\n",
      "Processed: 883.txt\n",
      "Processed: 129.txt\n",
      "Processed: 897.txt\n",
      "Processed: 667.txt\n",
      "Processed: 101.txt\n",
      "Processed: 115.txt\n",
      "Processed: 673.txt\n",
      "Processed: 840.txt\n",
      "Processed: 698.txt\n",
      "Processed: 854.txt\n",
      "Processed: 14.txt\n",
      "Processed: 868.txt\n",
      "Processed: 28.txt\n",
      "Processed: 713.txt\n",
      "Processed: 707.txt\n",
      "Processed: 934.txt\n",
      "Processed: 920.txt\n",
      "Processed: 908.txt\n",
      "Processed: 539.txt\n",
      "Processed: 277.txt\n",
      "Processed: 511.txt\n",
      "Processed: 505.txt\n",
      "Processed: 263.txt\n",
      "Processed: 288.txt\n",
      "Processed: 513.txt\n",
      "Processed: 275.txt\n",
      "Processed: 261.txt\n",
      "Processed: 507.txt\n",
      "Processed: 249.txt\n",
      "Processed: 936.txt\n",
      "Processed: 922.txt\n",
      "Processed: 711.txt\n",
      "Processed: 705.txt\n",
      "Processed: 739.txt\n",
      "Processed: 842.txt\n",
      "Processed: 16.txt\n",
      "Processed: 856.txt\n",
      "Processed: 103.txt\n",
      "Processed: 665.txt\n",
      "Processed: 671.txt\n",
      "Processed: 117.txt\n",
      "Processed: 881.txt\n",
      "Processed: 659.txt\n",
      "Processed: 895.txt\n",
      "Processed: 498.txt\n",
      "Processed: 467.txt\n",
      "Processed: 301.txt\n",
      "Processed: 315.txt\n",
      "Processed: 473.txt\n",
      "Processed: 329.txt\n",
      "Processed: 328.txt\n",
      "Processed: 314.txt\n",
      "Processed: 472.txt\n",
      "Processed: 466.txt\n",
      "Processed: 300.txt\n",
      "Processed: 499.txt\n",
      "Processed: 894.txt\n",
      "Processed: 658.txt\n",
      "Processed: 880.txt\n",
      "Processed: 670.txt\n",
      "Processed: 116.txt\n",
      "Processed: 102.txt\n",
      "Processed: 664.txt\n",
      "Processed: 857.txt\n",
      "Processed: 17.txt\n",
      "Processed: 843.txt\n",
      "Processed: 738.txt\n",
      "Processed: 704.txt\n",
      "Processed: 710.txt\n",
      "Processed: 923.txt\n",
      "Processed: 937.txt\n",
      "Processed: 248.txt\n",
      "Processed: 260.txt\n",
      "Processed: 506.txt\n",
      "Processed: 512.txt\n",
      "Processed: 274.txt\n",
      "Processed: 258.txt\n",
      "Processed: 270.txt\n",
      "Processed: 516.txt\n",
      "Processed: 502.txt\n",
      "Processed: 264.txt\n",
      "Processed: 933.txt\n",
      "Processed: 927.txt\n",
      "Processed: 728.txt\n",
      "Processed: 714.txt\n",
      "Processed: 700.txt\n",
      "Processed: 847.txt\n",
      "Processed: 13.txt\n",
      "Processed: 853.txt\n",
      "Processed: 884.txt\n",
      "Processed: 890.txt\n",
      "Processed: 648.txt\n",
      "Processed: 660.txt\n",
      "Processed: 106.txt\n",
      "Processed: 112.txt\n",
      "Processed: 674.txt\n",
      "Processed: 489.txt\n",
      "Processed: 338.txt\n",
      "Processed: 304.txt\n",
      "Processed: 462.txt\n",
      "Processed: 476.txt\n",
      "Processed: 310.txt\n",
      "Processed: 477.txt\n",
      "Processed: 311.txt\n",
      "Processed: 305.txt\n",
      "Processed: 463.txt\n",
      "Processed: 339.txt\n",
      "Processed: 488.txt\n",
      "Processed: 113.txt\n",
      "Processed: 675.txt\n",
      "Processed: 661.txt\n",
      "Processed: 107.txt\n",
      "Processed: 649.txt\n",
      "Processed: 891.txt\n",
      "Processed: 885.txt\n",
      "Processed: 852.txt\n",
      "Processed: 12.txt\n",
      "Processed: 846.txt\n",
      "Processed: 701.txt\n",
      "Processed: 715.txt\n",
      "Processed: 729.txt\n",
      "Processed: 926.txt\n",
      "Processed: 932.txt\n",
      "Processed: 503.txt\n",
      "Processed: 265.txt\n",
      "Processed: 271.txt\n",
      "Processed: 517.txt\n",
      "Processed: 259.txt\n",
      "Processed: 298.txt\n",
      "Processed: 529.txt\n",
      "Processed: 267.txt\n",
      "Processed: 501.txt\n",
      "Processed: 515.txt\n",
      "Processed: 273.txt\n",
      "Processed: 924.txt\n",
      "Processed: 930.txt\n",
      "Processed: 918.txt\n",
      "Processed: 703.txt\n",
      "Processed: 717.txt\n",
      "Processed: 688.txt\n",
      "Processed: 10.txt\n",
      "Processed: 850.txt\n",
      "Processed: 844.txt\n",
      "Processed: 38.txt\n",
      "Processed: 878.txt\n",
      "Processed: 893.txt\n",
      "Processed: 139.txt\n",
      "Processed: 887.txt\n",
      "Processed: 677.txt\n",
      "Processed: 111.txt\n",
      "Processed: 105.txt\n",
      "Processed: 663.txt\n",
      "Processed: 449.txt\n",
      "Processed: 313.txt\n",
      "Processed: 475.txt\n",
      "Processed: 461.txt\n",
      "Processed: 307.txt\n",
      "Processed: 460.txt\n",
      "Processed: 306.txt\n",
      "Processed: 312.txt\n",
      "Processed: 474.txt\n",
      "Processed: 448.txt\n",
      "Processed: 104.txt\n",
      "Processed: 662.txt\n",
      "Processed: 676.txt\n",
      "Processed: 110.txt\n",
      "Processed: 138.txt\n",
      "Processed: 886.txt\n",
      "Processed: 892.txt\n",
      "Processed: 879.txt\n",
      "Processed: 39.txt\n",
      "Processed: 845.txt\n",
      "Processed: 851.txt\n",
      "Processed: 11.txt\n",
      "Processed: 689.txt\n",
      "Processed: 716.txt\n",
      "Processed: 702.txt\n",
      "Processed: 919.txt\n",
      "Processed: 931.txt\n",
      "Processed: 925.txt\n",
      "Processed: 514.txt\n",
      "Processed: 272.txt\n",
      "Processed: 266.txt\n",
      "Processed: 500.txt\n",
      "Processed: 528.txt\n",
      "Processed: 299.txt\n",
      "Processed: 598.txt\n",
      "Processed: 201.txt\n",
      "Processed: 567.txt\n",
      "Processed: 573.txt\n",
      "Processed: 215.txt\n",
      "Processed: 229.txt\n",
      "Processed: 942.txt\n",
      "Processed: 956.txt\n",
      "Processed: 765.txt\n",
      "Processed: 771.txt\n",
      "Processed: 759.txt\n",
      "Processed: 981.txt\n",
      "Processed: 995.txt\n",
      "Processed: 76.txt\n",
      "Processed: 836.txt\n",
      "Processed: 188.txt\n",
      "Processed: 62.txt\n",
      "Processed: 822.txt\n",
      "Processed: 89.txt\n",
      "Processed: 611.txt\n",
      "Processed: 177.txt\n",
      "Processed: 163.txt\n",
      "Processed: 605.txt\n",
      "Processed: 639.txt\n",
      "Processed: 375.txt\n",
      "Processed: 413.txt\n",
      "Processed: 407.txt\n",
      "Processed: 361.txt\n",
      "Processed: 349.txt\n",
      "Processed: 348.txt\n",
      "Processed: 406.txt\n",
      "Processed: 360.txt\n",
      "Processed: 374.txt\n",
      "Processed: 412.txt\n",
      "Processed: 638.txt\n",
      "Processed: 162.txt\n",
      "Processed: 604.txt\n",
      "Processed: 610.txt\n",
      "Processed: 88.txt\n",
      "Processed: 176.txt\n",
      "Processed: 823.txt\n",
      "Processed: 63.txt\n",
      "Processed: 837.txt\n",
      "Processed: 77.txt\n",
      "Processed: 189.txt\n",
      "Processed: 994.txt\n",
      "Processed: 980.txt\n",
      "Processed: 758.txt\n",
      "Processed: 770.txt\n",
      "Processed: 764.txt\n",
      "Processed: 957.txt\n",
      "Processed: 943.txt\n",
      "Processed: 228.txt\n",
      "Processed: 572.txt\n",
      "Processed: 214.txt\n",
      "Processed: 200.txt\n",
      "Processed: 566.txt\n",
      "Processed: 599.txt\n",
      "Processed: 216.txt\n",
      "Processed: 570.txt\n",
      "Processed: 564.txt\n",
      "Processed: 202.txt\n",
      "Processed: 558.txt\n",
      "Processed: 969.txt\n",
      "Processed: 955.txt\n",
      "Processed: 941.txt\n",
      "Processed: 799.txt\n",
      "Processed: 772.txt\n",
      "Processed: 766.txt\n",
      "Processed: 996.txt\n",
      "Processed: 982.txt\n",
      "Processed: 49.txt\n",
      "Processed: 809.txt\n",
      "Processed: 61.txt\n",
      "Processed: 821.txt\n",
      "Processed: 75.txt\n",
      "Processed: 835.txt\n",
      "Processed: 606.txt\n",
      "Processed: 160.txt\n",
      "Processed: 174.txt\n",
      "Processed: 612.txt\n",
      "Processed: 148.txt\n",
      "Processed: 389.txt\n",
      "Processed: 362.txt\n",
      "Processed: 404.txt\n",
      "Processed: 410.txt\n",
      "Processed: 376.txt\n",
      "Processed: 438.txt\n",
      "Processed: 439.txt\n",
      "Processed: 411.txt\n",
      "Processed: 377.txt\n",
      "Processed: 363.txt\n",
      "Processed: 405.txt\n",
      "Processed: 388.txt\n",
      "Processed: 149.txt\n",
      "Processed: 175.txt\n",
      "Processed: 613.txt\n",
      "Processed: 607.txt\n",
      "Processed: 161.txt\n",
      "Processed: 834.txt\n",
      "Processed: 74.txt\n",
      "Processed: 820.txt\n",
      "Processed: 60.txt\n",
      "Processed: 808.txt\n",
      "Processed: 48.txt\n",
      "Processed: 983.txt\n",
      "Processed: 997.txt\n",
      "Processed: 767.txt\n",
      "Processed: 773.txt\n",
      "Processed: 798.txt\n",
      "Processed: 940.txt\n",
      "Processed: 954.txt\n",
      "Processed: 968.txt\n",
      "Processed: 559.txt\n",
      "Processed: 565.txt\n",
      "Processed: 203.txt\n",
      "Processed: 217.txt\n",
      "Processed: 571.txt\n",
      "Processed: 549.txt\n",
      "Processed: 575.txt\n",
      "Processed: 213.txt\n",
      "Processed: 207.txt\n",
      "Processed: 561.txt\n",
      "Processed: 950.txt\n",
      "Processed: 788.txt\n",
      "Processed: 944.txt\n",
      "Processed: 978.txt\n",
      "Processed: 993.txt\n",
      "Processed: 987.txt\n",
      "Processed: 777.txt\n",
      "Processed: 763.txt\n",
      "Processed: 64.txt\n",
      "Processed: 824.txt\n",
      "Processed: 70.txt\n",
      "Processed: 830.txt\n",
      "Processed: 58.txt\n",
      "Processed: 818.txt\n",
      "Processed: 159.txt\n",
      "Processed: 165.txt\n",
      "Processed: 603.txt\n",
      "Processed: 617.txt\n",
      "Processed: 171.txt\n",
      "Processed: 398.txt\n",
      "Processed: 429.txt\n",
      "Processed: 401.txt\n",
      "Processed: 367.txt\n",
      "Processed: 373.txt\n",
      "Processed: 415.txt\n",
      "Processed: 372.txt\n",
      "Processed: 414.txt\n",
      "Processed: 400.txt\n",
      "Processed: 366.txt\n",
      "Processed: 428.txt\n",
      "Processed: 399.txt\n",
      "Processed: 616.txt\n",
      "Processed: 170.txt\n",
      "Processed: 164.txt\n",
      "Processed: 602.txt\n",
      "Processed: 158.txt\n",
      "Processed: 819.txt\n",
      "Processed: 59.txt\n",
      "Processed: 831.txt\n",
      "Processed: 71.txt\n",
      "Processed: 825.txt\n",
      "Processed: 65.txt\n",
      "Processed: 762.txt\n",
      "Processed: 776.txt\n",
      "Processed: 986.txt\n",
      "Processed: 992.txt\n",
      "Processed: 979.txt\n",
      "Processed: 945.txt\n",
      "Processed: 789.txt\n",
      "Processed: 951.txt\n",
      "Processed: 206.txt\n",
      "Processed: 560.txt\n",
      "Processed: 574.txt\n",
      "Processed: 212.txt\n",
      "Processed: 548.txt\n",
      "Processed: 589.txt\n",
      "Processed: 238.txt\n",
      "Processed: 562.txt\n",
      "Processed: 204.txt\n",
      "Processed: 210.txt\n",
      "Processed: 576.txt\n",
      "Processed: 947.txt\n",
      "Processed: 953.txt\n",
      "Processed: 984.txt\n",
      "Processed: 748.txt\n",
      "Processed: 990.txt\n",
      "Processed: 760.txt\n",
      "Processed: 774.txt\n",
      "Processed: 73.txt\n",
      "Processed: 833.txt\n",
      "Processed: 67.txt\n",
      "Processed: 827.txt\n",
      "Processed: 199.txt\n",
      "Processed: 9.txt\n",
      "Processed: 628.txt\n",
      "Processed: 172.txt\n",
      "Processed: 614.txt\n",
      "Processed: 600.txt\n",
      "Processed: 98.txt\n",
      "Processed: 166.txt\n",
      "Processed: 358.txt\n",
      "Processed: 416.txt\n",
      "Processed: 370.txt\n",
      "Processed: 364.txt\n",
      "Processed: 402.txt\n",
      "Processed: 365.txt\n",
      "Processed: 403.txt\n",
      "Processed: 417.txt\n",
      "Processed: 371.txt\n",
      "Processed: 359.txt\n",
      "Processed: 99.txt\n",
      "Processed: 601.txt\n",
      "Processed: 167.txt\n",
      "Processed: 173.txt\n",
      "Processed: 615.txt\n",
      "Processed: 629.txt\n",
      "Processed: 8.txt\n",
      "Processed: 826.txt\n",
      "Processed: 66.txt\n",
      "Processed: 198.txt\n",
      "Processed: 832.txt\n",
      "Processed: 72.txt\n",
      "Processed: 775.txt\n",
      "Processed: 761.txt\n",
      "Processed: 991.txt\n",
      "Processed: 749.txt\n",
      "Processed: 985.txt\n",
      "Processed: 952.txt\n",
      "Processed: 946.txt\n",
      "Processed: 211.txt\n",
      "Processed: 577.txt\n",
      "Processed: 563.txt\n",
      "Processed: 205.txt\n",
      "Processed: 239.txt\n",
      "Processed: 588.txt\n",
      "Processed: 585.txt\n",
      "Processed: 591.txt\n",
      "Processed: 220.txt\n",
      "Processed: 546.txt\n",
      "Processed: 552.txt\n",
      "Processed: 234.txt\n",
      "Processed: 208.txt\n",
      "Processed: 787.txt\n",
      "Processed: 793.txt\n",
      "Processed: 963.txt\n",
      "Processed: 977.txt\n",
      "Processed: 744.txt\n",
      "Processed: 750.txt\n",
      "Processed: 988.txt\n",
      "Processed: 778.txt\n",
      "Processed: 195.txt\n",
      "Processed: 181.txt\n",
      "Processed: 57.txt\n",
      "Processed: 817.txt\n",
      "Processed: 5.txt\n",
      "Processed: 43.txt\n",
      "Processed: 803.txt\n",
      "Processed: 630.txt\n",
      "Processed: 156.txt\n",
      "Processed: 142.txt\n",
      "Processed: 624.txt\n",
      "Processed: 94.txt\n",
      "Processed: 80.txt\n",
      "Processed: 618.txt\n",
      "Processed: 397.txt\n",
      "Processed: 383.txt\n",
      "Processed: 354.txt\n",
      "Processed: 432.txt\n",
      "Processed: 426.txt\n",
      "Processed: 340.txt\n",
      "Processed: 368.txt\n",
      "Processed: 369.txt\n",
      "Processed: 427.txt\n",
      "Processed: 341.txt\n",
      "Processed: 355.txt\n",
      "Processed: 433.txt\n",
      "Processed: 382.txt\n",
      "Processed: 396.txt\n",
      "Processed: 619.txt\n",
      "Processed: 81.txt\n",
      "Processed: 95.txt\n",
      "Processed: 143.txt\n",
      "Processed: 625.txt\n",
      "Processed: 631.txt\n",
      "Processed: 157.txt\n",
      "Processed: 802.txt\n",
      "Processed: 42.txt\n",
      "Processed: 816.txt\n",
      "Processed: 56.txt\n",
      "Processed: 4.txt\n",
      "Processed: 180.txt\n",
      "Processed: 194.txt\n",
      "Processed: 779.txt\n",
      "Processed: 989.txt\n",
      "Processed: 751.txt\n",
      "Processed: 745.txt\n",
      "Processed: 976.txt\n",
      "Processed: 962.txt\n",
      "Processed: 792.txt\n",
      "Processed: 786.txt\n",
      "Processed: 209.txt\n",
      "Processed: 553.txt\n",
      "Processed: 235.txt\n",
      "Processed: 221.txt\n",
      "Processed: 547.txt\n",
      "Processed: 590.txt\n",
      "Processed: 584.txt\n",
      "Processed: 592.txt\n",
      "Processed: 586.txt\n",
      "Processed: 237.txt\n",
      "Processed: 551.txt\n",
      "Processed: 545.txt\n",
      "Processed: 223.txt\n",
      "Processed: 579.txt\n",
      "Processed: 948.txt\n",
      "Processed: 790.txt\n",
      "Processed: 784.txt\n",
      "Processed: 974.txt\n",
      "Processed: 960.txt\n",
      "Processed: 753.txt\n",
      "Processed: 747.txt\n",
      "Processed: 182.txt\n",
      "Processed: 196.txt\n",
      "Processed: 68.txt\n",
      "Processed: 828.txt\n",
      "Processed: 40.txt\n",
      "Processed: 800.txt\n",
      "Processed: 6.txt\n",
      "Processed: 54.txt\n",
      "Processed: 814.txt\n",
      "Processed: 627.txt\n",
      "Processed: 141.txt\n",
      "Processed: 155.txt\n",
      "Processed: 633.txt\n",
      "Processed: 83.txt\n",
      "Processed: 169.txt\n",
      "Processed: 97.txt\n",
      "Processed: 380.txt\n",
      "Processed: 394.txt\n",
      "Processed: 343.txt\n",
      "Processed: 425.txt\n",
      "Processed: 431.txt\n",
      "Processed: 357.txt\n",
      "Processed: 419.txt\n",
      "Processed: 418.txt\n",
      "Processed: 430.txt\n",
      "Processed: 356.txt\n",
      "Processed: 342.txt\n",
      "Processed: 424.txt\n",
      "Processed: 395.txt\n",
      "Processed: 381.txt\n",
      "Processed: 168.txt\n",
      "Processed: 96.txt\n",
      "Processed: 82.txt\n",
      "Processed: 154.txt\n",
      "Processed: 632.txt\n",
      "Processed: 626.txt\n",
      "Processed: 140.txt\n",
      "Processed: 7.txt\n",
      "Processed: 815.txt\n",
      "Processed: 55.txt\n",
      "Processed: 801.txt\n",
      "Processed: 41.txt\n",
      "Processed: 197.txt\n",
      "Processed: 829.txt\n",
      "Processed: 69.txt\n",
      "Processed: 183.txt\n",
      "Processed: 746.txt\n",
      "Processed: 752.txt\n",
      "Processed: 961.txt\n",
      "Processed: 975.txt\n",
      "Processed: 785.txt\n",
      "Processed: 791.txt\n",
      "Processed: 949.txt\n",
      "Processed: 578.txt\n",
      "Processed: 544.txt\n",
      "Processed: 222.txt\n",
      "Processed: 236.txt\n",
      "Processed: 550.txt\n",
      "Processed: 587.txt\n",
      "Processed: 593.txt\n",
      "Processed: 597.txt\n",
      "Processed: 583.txt\n",
      "Processed: 568.txt\n",
      "Processed: 554.txt\n",
      "Processed: 232.txt\n",
      "Processed: 226.txt\n",
      "Processed: 540.txt\n",
      "Processed: 971.txt\n",
      "Processed: 965.txt\n",
      "Processed: 795.txt\n",
      "Processed: 959.txt\n",
      "Processed: 781.txt\n",
      "Processed: 756.txt\n",
      "Processed: 742.txt\n",
      "Processed: 45.txt\n",
      "Processed: 805.txt\n",
      "Processed: 51.txt\n",
      "Processed: 811.txt\n",
      "Processed: 3.txt\n",
      "Processed: 187.txt\n",
      "Processed: 79.txt\n",
      "Processed: 839.txt\n",
      "Processed: 193.txt\n",
      "Processed: 178.txt\n",
      "Processed: 86.txt\n",
      "Processed: 92.txt\n",
      "Processed: 144.txt\n",
      "Processed: 622.txt\n",
      "Processed: 636.txt\n",
      "Processed: 150.txt\n",
      "Processed: 385.txt\n",
      "Processed: 391.txt\n",
      "Processed: 408.txt\n",
      "Processed: 420.txt\n",
      "Processed: 346.txt\n",
      "Processed: 352.txt\n",
      "Processed: 434.txt\n",
      "Processed: 353.txt\n",
      "Processed: 435.txt\n",
      "Processed: 421.txt\n",
      "Processed: 347.txt\n",
      "Processed: 409.txt\n",
      "Processed: 390.txt\n",
      "Processed: 384.txt\n",
      "Processed: 637.txt\n",
      "Processed: 151.txt\n",
      "Processed: 145.txt\n",
      "Processed: 623.txt\n",
      "Processed: 93.txt\n",
      "Processed: 179.txt\n",
      "Processed: 87.txt\n",
      "Processed: 192.txt\n",
      "Processed: 186.txt\n",
      "Processed: 838.txt\n",
      "Processed: 78.txt\n",
      "Processed: 810.txt\n",
      "Processed: 50.txt\n",
      "Processed: 2.txt\n",
      "Processed: 804.txt\n",
      "Processed: 44.txt\n",
      "Processed: 743.txt\n",
      "Processed: 757.txt\n",
      "Processed: 780.txt\n",
      "Processed: 958.txt\n",
      "Processed: 794.txt\n",
      "Processed: 964.txt\n",
      "Processed: 970.txt\n",
      "Processed: 227.txt\n",
      "Processed: 541.txt\n",
      "Processed: 555.txt\n",
      "Processed: 233.txt\n",
      "Processed: 569.txt\n",
      "Processed: 582.txt\n",
      "Processed: 596.txt\n",
      "Processed: 580.txt\n",
      "Processed: 594.txt\n",
      "Processed: 1000.txt\n",
      "Processed: 219.txt\n",
      "Processed: 543.txt\n",
      "Processed: 225.txt\n",
      "Processed: 231.txt\n",
      "Processed: 557.txt\n",
      "Processed: 966.txt\n",
      "Processed: 972.txt\n",
      "Processed: 782.txt\n",
      "Processed: 796.txt\n",
      "Processed: 769.txt\n",
      "Processed: 741.txt\n",
      "Processed: 999.txt\n",
      "Processed: 755.txt\n",
      "Processed: 52.txt\n",
      "Processed: 812.txt\n",
      "Processed: 46.txt\n",
      "Processed: 806.txt\n",
      "Processed: 190.txt\n",
      "Processed: 184.txt\n",
      "Processed: 609.txt\n",
      "Processed: 91.txt\n",
      "Processed: 85.txt\n",
      "Processed: 153.txt\n",
      "Processed: 635.txt\n",
      "Processed: 621.txt\n",
      "Processed: 147.txt\n",
      "Processed: 392.txt\n",
      "Processed: 386.txt\n",
      "Processed: 379.txt\n",
      "Processed: 437.txt\n",
      "Processed: 351.txt\n",
      "Processed: 345.txt\n",
      "Processed: 423.txt\n",
      "Processed: 344.txt\n",
      "Processed: 422.txt\n",
      "Processed: 436.txt\n",
      "Processed: 350.txt\n",
      "Processed: 378.txt\n",
      "Processed: 387.txt\n",
      "Processed: 393.txt\n",
      "Processed: 620.txt\n",
      "Processed: 146.txt\n",
      "Processed: 152.txt\n",
      "Processed: 634.txt\n",
      "Processed: 84.txt\n",
      "Processed: 90.txt\n",
      "Processed: 608.txt\n",
      "Processed: 185.txt\n",
      "Processed: 191.txt\n",
      "Processed: 807.txt\n",
      "Processed: 47.txt\n",
      "Processed: 1.txt\n",
      "Processed: 813.txt\n",
      "Processed: 53.txt\n",
      "Processed: 754.txt\n",
      "Processed: 998.txt\n",
      "Processed: 740.txt\n",
      "Processed: 768.txt\n",
      "Processed: 797.txt\n",
      "Processed: 783.txt\n",
      "Processed: 973.txt\n",
      "Processed: 967.txt\n",
      "Processed: 230.txt\n",
      "Processed: 556.txt\n",
      "Processed: 542.txt\n",
      "Processed: 224.txt\n",
      "Processed: 218.txt\n",
      "Processed: 595.txt\n",
      "Processed: 581.txt\n",
      "Processed: 280.txt\n",
      "Processed: 294.txt\n",
      "Processed: 525.txt\n",
      "Processed: 243.txt\n",
      "Processed: 257.txt\n",
      "Processed: 531.txt\n",
      "Processed: 519.txt\n",
      "Processed: 928.txt\n",
      "Processed: 900.txt\n",
      "Processed: 914.txt\n",
      "Processed: 727.txt\n",
      "Processed: 733.txt\n",
      "Processed: 690.txt\n",
      "Processed: 848.txt\n",
      "Processed: 684.txt\n",
      "Processed: 34.txt\n",
      "Processed: 874.txt\n",
      "Processed: 20.txt\n",
      "Processed: 860.txt\n",
      "Processed: 135.txt\n",
      "Processed: 653.txt\n",
      "Processed: 647.txt\n",
      "Processed: 121.txt\n",
      "Processed: 109.txt\n",
      "Processed: 492.txt\n",
      "Processed: 486.txt\n",
      "Processed: 451.txt\n",
      "Processed: 337.txt\n",
      "Processed: 323.txt\n",
      "Processed: 445.txt\n",
      "Processed: 479.txt\n",
      "Processed: 478.txt\n",
      "Processed: 322.txt\n",
      "Processed: 444.txt\n",
      "Processed: 450.txt\n",
      "Processed: 336.txt\n",
      "Processed: 487.txt\n",
      "Processed: 493.txt\n",
      "Processed: 108.txt\n",
      "Processed: 646.txt\n",
      "Processed: 120.txt\n",
      "Processed: 134.txt\n",
      "Processed: 652.txt\n",
      "Processed: 861.txt\n",
      "Processed: 21.txt\n",
      "Processed: 875.txt\n",
      "Processed: 35.txt\n",
      "Processed: 685.txt\n",
      "Processed: 849.txt\n",
      "Processed: 691.txt\n",
      "Processed: 732.txt\n",
      "Processed: 726.txt\n",
      "Processed: 915.txt\n",
      "Processed: 901.txt\n",
      "Processed: 929.txt\n",
      "Processed: 518.txt\n",
      "Processed: 256.txt\n",
      "Processed: 530.txt\n",
      "Processed: 524.txt\n",
      "Processed: 242.txt\n",
      "Processed: 295.txt\n",
      "Processed: 281.txt\n",
      "Processed: 297.txt\n",
      "Processed: 283.txt\n",
      "Processed: 532.txt\n",
      "Processed: 254.txt\n",
      "Processed: 240.txt\n",
      "Processed: 526.txt\n",
      "Processed: 268.txt\n",
      "Processed: 917.txt\n",
      "Processed: 903.txt\n",
      "Processed: 730.txt\n",
      "Processed: 724.txt\n",
      "Processed: 718.txt\n",
      "Processed: 687.txt\n",
      "Processed: 693.txt\n",
      "Processed: 23.txt\n",
      "Processed: 863.txt\n",
      "Processed: 37.txt\n",
      "Processed: 877.txt\n",
      "Processed: 122.txt\n",
      "Processed: 644.txt\n",
      "Processed: 888.txt\n",
      "Processed: 650.txt\n",
      "Processed: 136.txt\n",
      "Processed: 678.txt\n",
      "Processed: 485.txt\n",
      "Processed: 491.txt\n",
      "Processed: 446.txt\n",
      "Processed: 320.txt\n",
      "Processed: 334.txt\n",
      "Processed: 452.txt\n",
      "Processed: 308.txt\n",
      "Processed: 309.txt\n",
      "Processed: 335.txt\n",
      "Processed: 453.txt\n",
      "Processed: 447.txt\n",
      "Processed: 321.txt\n",
      "Processed: 490.txt\n",
      "Processed: 484.txt\n",
      "Processed: 679.txt\n",
      "Processed: 651.txt\n",
      "Processed: 889.txt\n",
      "Processed: 137.txt\n",
      "Processed: 123.txt\n",
      "Processed: 645.txt\n",
      "Processed: 876.txt\n",
      "Processed: 36.txt\n",
      "Processed: 862.txt\n",
      "Processed: 22.txt\n",
      "Processed: 692.txt\n",
      "Processed: 686.txt\n",
      "Processed: 719.txt\n",
      "Processed: 725.txt\n",
      "Processed: 731.txt\n",
      "Processed: 902.txt\n",
      "Processed: 916.txt\n",
      "Processed: 269.txt\n",
      "Processed: 241.txt\n",
      "Processed: 527.txt\n",
      "Processed: 533.txt\n",
      "Processed: 255.txt\n",
      "Processed: 282.txt\n",
      "Processed: 296.txt\n",
      "Processed: 292.txt\n",
      "Processed: 286.txt\n",
      "Processed: 279.txt\n",
      "Processed: 251.txt\n",
      "Processed: 537.txt\n",
      "Processed: 523.txt\n",
      "Processed: 245.txt\n",
      "Processed: 912.txt\n",
      "Processed: 906.txt\n",
      "Processed: 709.txt\n",
      "Processed: 735.txt\n",
      "Processed: 721.txt\n",
      "Processed: 26.txt\n",
      "Processed: 866.txt\n",
      "Processed: 32.txt\n",
      "Processed: 872.txt\n",
      "Processed: 682.txt\n",
      "Processed: 696.txt\n",
      "Processed: 669.txt\n",
      "Processed: 899.txt\n",
      "Processed: 641.txt\n",
      "Processed: 127.txt\n",
      "Processed: 133.txt\n",
      "Processed: 655.txt\n",
      "Processed: 480.txt\n",
      "Processed: 494.txt\n",
      "Processed: 319.txt\n",
      "Processed: 325.txt\n",
      "Processed: 443.txt\n",
      "Processed: 457.txt\n",
      "Processed: 331.txt\n",
      "Processed: 456.txt\n",
      "Processed: 330.txt\n",
      "Processed: 324.txt\n",
      "Processed: 442.txt\n",
      "Processed: 318.txt\n",
      "Processed: 495.txt\n",
      "Processed: 481.txt\n",
      "Processed: 132.txt\n",
      "Processed: 654.txt\n",
      "Processed: 640.txt\n",
      "Processed: 898.txt\n",
      "Processed: 126.txt\n",
      "Processed: 668.txt\n",
      "Processed: 697.txt\n",
      "Processed: 683.txt\n",
      "Processed: 873.txt\n",
      "Processed: 33.txt\n",
      "Processed: 867.txt\n",
      "Processed: 27.txt\n",
      "Processed: 720.txt\n",
      "Processed: 734.txt\n",
      "Processed: 708.txt\n",
      "Processed: 907.txt\n",
      "Processed: 913.txt\n",
      "Processed: 522.txt\n",
      "Processed: 244.txt\n",
      "Processed: 250.txt\n",
      "Processed: 536.txt\n",
      "Processed: 278.txt\n",
      "Processed: 287.txt\n",
      "Processed: 293.txt\n",
      "Processed: 285.txt\n",
      "Processed: 291.txt\n",
      "Processed: 508.txt\n",
      "Processed: 246.txt\n",
      "Processed: 520.txt\n",
      "Processed: 534.txt\n",
      "Processed: 252.txt\n",
      "Processed: 905.txt\n",
      "Processed: 911.txt\n",
      "Processed: 939.txt\n",
      "Processed: 722.txt\n",
      "Processed: 736.txt\n",
      "Processed: 31.txt\n",
      "Processed: 871.txt\n",
      "Processed: 25.txt\n",
      "Processed: 865.txt\n",
      "Processed: 695.txt\n",
      "Processed: 681.txt\n",
      "Processed: 19.txt\n",
      "Processed: 859.txt\n",
      "Processed: 118.txt\n",
      "Processed: 656.txt\n",
      "Processed: 130.txt\n",
      "Processed: 124.txt\n",
      "Processed: 642.txt\n",
      "Processed: 497.txt\n",
      "Processed: 483.txt\n",
      "Processed: 468.txt\n",
      "Processed: 332.txt\n",
      "Processed: 454.txt\n",
      "Processed: 440.txt\n",
      "Processed: 326.txt\n",
      "Processed: 441.txt\n",
      "Processed: 327.txt\n",
      "Processed: 333.txt\n",
      "Processed: 455.txt\n",
      "Processed: 469.txt\n",
      "Processed: 482.txt\n",
      "Processed: 496.txt\n",
      "Processed: 125.txt\n",
      "Processed: 643.txt\n",
      "Processed: 657.txt\n",
      "Processed: 131.txt\n",
      "Processed: 119.txt\n",
      "Processed: 858.txt\n",
      "Processed: 18.txt\n",
      "Processed: 680.txt\n",
      "Processed: 694.txt\n",
      "Processed: 864.txt\n",
      "Processed: 24.txt\n",
      "Processed: 870.txt\n",
      "Processed: 30.txt\n",
      "Processed: 737.txt\n",
      "Processed: 723.txt\n",
      "Processed: 938.txt\n",
      "Processed: 910.txt\n",
      "Processed: 904.txt\n",
      "Processed: 535.txt\n",
      "Processed: 253.txt\n",
      "Processed: 247.txt\n",
      "Processed: 521.txt\n",
      "Processed: 509.txt\n",
      "Processed: 290.txt\n",
      "Processed: 284.txt\n",
      "\n",
      "Conversion complete! Results saved to infotech.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import csv\n",
    "\n",
    "def read_and_convert_file(file_path):\n",
    "    \"\"\"\n",
    "    Read a single text file and convert its content to Preeti\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the input text file\n",
    "    \n",
    "    Returns:\n",
    "        list: List of converted words/lines\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read the file\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read().strip()\n",
    "            \n",
    "        # Split content into lines or words\n",
    "        lines = content.split('\\n')\n",
    "        \n",
    "        # Convert each line\n",
    "        converted_lines = []\n",
    "        for line in lines:\n",
    "            # Normalize and convert the line\n",
    "            normalized_line = normalize_unicode(line)\n",
    "            preeti_line = convert_to_preeti(normalized_line)\n",
    "            converted_lines.append((line, preeti_line))\n",
    "        \n",
    "        return converted_lines\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "def batch_convert_files(input_folder, output_csv):\n",
    "    \"\"\"\n",
    "    Convert all .txt files in a folder to Preeti and save to CSV\n",
    "    \n",
    "    Args:\n",
    "        input_folder (str): Path to folder containing .txt files\n",
    "        output_csv (str): Path to save the output CSV file\n",
    "    \"\"\"\n",
    "    # Find all .txt files in the input folder\n",
    "    txt_files = glob.glob(os.path.join(input_folder, '*.txt'))\n",
    "    \n",
    "    # Prepare CSV for writing\n",
    "    with open(output_csv, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "        \n",
    "        # Write CSV header\n",
    "        csv_writer.writerow(['File Name', 'Original Text', 'Preeti Text'])\n",
    "        \n",
    "        # Process each file\n",
    "        for file_path in txt_files:\n",
    "            # Get just the filename\n",
    "            filename = os.path.basename(file_path)\n",
    "            \n",
    "            # Convert file contents\n",
    "            converted_lines = read_and_convert_file(file_path)\n",
    "            \n",
    "            # Write converted lines to CSV\n",
    "            for original, preeti in converted_lines:\n",
    "                csv_writer.writerow([filename, original, preeti])\n",
    "            \n",
    "            print(f\"Processed: {filename}\")\n",
    "    \n",
    "    print(f\"\\nConversion complete! Results saved to {output_csv}\")\n",
    "\n",
    "# Interactive usage in Jupyter or Python script\n",
    "def main():\n",
    "    # Prompt for input folder and output file\n",
    "    input_folder = input(\"Enter the path to the folder containing .txt files: \")\n",
    "    output_csv = input(\"Enter the path to save the output CSV file: \")\n",
    "    \n",
    "    # Validate input folder\n",
    "    if not os.path.isdir(input_folder):\n",
    "        print(\"Error: Invalid folder path\")\n",
    "        return\n",
    "    \n",
    "    # Perform batch conversion\n",
    "    batch_convert_files(input_folder, output_csv)\n",
    "\n",
    "# For Jupyter Lab with widgets\n",
    "def create_conversion_widget():\n",
    "    import ipywidgets as widgets\n",
    "    from IPython.display import display\n",
    "    \n",
    "    # Input folder selection\n",
    "    input_folder_widget = widgets.Text(\n",
    "        value='',\n",
    "        placeholder='Enter path to input folder',\n",
    "        description='Input Folder:',\n",
    "        style={'description_width': 'initial'},\n",
    "        layout={'width': '500px'}\n",
    "    )\n",
    "    \n",
    "    # Output CSV file selection\n",
    "    output_file_widget = widgets.Text(\n",
    "        value='',\n",
    "        placeholder='Enter path to output CSV file',\n",
    "        description='Output CSV:',\n",
    "        style={'description_width': 'initial'},\n",
    "        layout={'width': '500px'}\n",
    "    )\n",
    "    \n",
    "    # Status output\n",
    "    status_widget = widgets.Output()\n",
    "    \n",
    "    def on_convert_clicked(b):\n",
    "        with status_widget:\n",
    "            status_widget.clear_output()\n",
    "            try:\n",
    "                print(\"Starting conversion...\")\n",
    "                batch_convert_files(input_folder_widget.value, output_file_widget.value)\n",
    "                print(\"Conversion completed successfully!\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error during conversion: {e}\")\n",
    "    \n",
    "    convert_button = widgets.Button(description=\"Convert Files\")\n",
    "    convert_button.on_click(on_convert_clicked)\n",
    "    \n",
    "    # Display widgets\n",
    "    display(widgets.HTML(\"<h2>Batch Unicode to Preeti Converter</h2>\"))\n",
    "    display(input_folder_widget)\n",
    "    display(output_file_widget)\n",
    "    display(convert_button)\n",
    "    display(status_widget)\n",
    "\n",
    "# Run methods based on environment\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4534ff70-589f-4c8c-a16c-28fbd88676a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean data remove empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a53ccd5-f8ba-4ccc-aa87-1aad84ffad30",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial DataFrame Shape: (8536, 3)\n",
      "Cleaned DataFrame Shape: (8536, 3)\n",
      "Rows Removed: 0\n",
      "Cleaned CSV saved to: infotech_cleaned.csv\n",
      "\n",
      "First few rows of cleaned DataFrame:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File Name</th>\n",
       "      <th>Original Text</th>\n",
       "      <th>Preeti Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>289.txt</td>\n",
       "      <td>कम्प्युटरबाट तेस्रो विश्वयुद्धअमेरिकाले तेस्रो...</td>\n",
       "      <td>sDKo'6/af6 t];|f] ljZjo'4cd]l/sfn] t];|f] ljZj...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>289.txt</td>\n",
       "      <td>अमेरिकाले तेस्रो विश्वयुद्ध जमिनमा नभएर इन्टरन...</td>\n",
       "      <td>cd]l/sfn] t];|f] ljZjo'4 hldgdf geP/ OG6/g]6 ;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>289.txt</td>\n",
       "      <td>अमेरिकामा साइबर आक्रमण जारी रहेको उनले बताए  प...</td>\n",
       "      <td>cd]l/sfdf ;fOa/ cfs|d0f hf/L /x]sf] pgn] atfP ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>504.txt</td>\n",
       "      <td>सामाजिक सञ्जालबाट अपराधफेसबुक र टि्वटरजस्ता सम...</td>\n",
       "      <td>;fdflhs ;`\\hfnaf6 ck/fwkm];a's / l6\\j6/h:tf ;d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>504.txt</td>\n",
       "      <td>फेसबुक र टि्वटरजस्ता समाजिक सञ्जालसँग जोडिएका ...</td>\n",
       "      <td>km];a's / l6\\j6/h:tf ;dflhs ;`\\hfn;Fu hf]l8Psf...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  File Name                                      Original Text  \\\n",
       "0   289.txt  कम्प्युटरबाट तेस्रो विश्वयुद्धअमेरिकाले तेस्रो...   \n",
       "1   289.txt  अमेरिकाले तेस्रो विश्वयुद्ध जमिनमा नभएर इन्टरन...   \n",
       "2   289.txt  अमेरिकामा साइबर आक्रमण जारी रहेको उनले बताए  प...   \n",
       "3   504.txt  सामाजिक सञ्जालबाट अपराधफेसबुक र टि्वटरजस्ता सम...   \n",
       "4   504.txt  फेसबुक र टि्वटरजस्ता समाजिक सञ्जालसँग जोडिएका ...   \n",
       "\n",
       "                                         Preeti Text  \n",
       "0  sDKo'6/af6 t];|f] ljZjo'4cd]l/sfn] t];|f] ljZj...  \n",
       "1  cd]l/sfn] t];|f] ljZjo'4 hldgdf geP/ OG6/g]6 ;...  \n",
       "2  cd]l/sfdf ;fOa/ cfs|d0f hf/L /x]sf] pgn] atfP ...  \n",
       "3  ;fdflhs ;`\\hfnaf6 ck/fwkm];a's / l6\\j6/h:tf ;d...  \n",
       "4  km];a's / l6\\j6/h:tf ;dflhs ;`\\hfn;Fu hf]l8Psf...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Data Cleaning Analysis ---\n",
      "Total Rows: 8536\n",
      "\n",
      "--- Original Text Analysis ---\n",
      "Original Text Length Distribution:\n",
      "count    8536.000000\n",
      "mean      282.691073\n",
      "std       213.929541\n",
      "min         4.000000\n",
      "25%       171.000000\n",
      "50%       262.000000\n",
      "75%       362.000000\n",
      "max      8873.000000\n",
      "Name: Original Text Length, dtype: float64\n",
      "\n",
      "--- Preeti Text Analysis ---\n",
      "Preeti Text Length Distribution:\n",
      "count    8536.000000\n",
      "mean      278.147376\n",
      "std       209.460347\n",
      "min         4.000000\n",
      "25%       169.000000\n",
      "50%       259.000000\n",
      "75%       355.000000\n",
      "max      8551.000000\n",
      "Name: Preeti Text Length, dtype: float64\n",
      "\n",
      "Unique Values:\n",
      "Unique Original Texts: 8380\n",
      "Unique Preeti Texts: 8378\n",
      "\n",
      "--- Tab Character Check ---\n",
      "Rows with Tab in Original Text: 2\n",
      "Rows with Tab in Preeti Text: 2\n",
      "\n",
      "--- Advanced Text Analysis ---\n",
      "Special Characters Distribution:\n",
      "Original Text Special Chars:\n",
      "count    8536.000000\n",
      "mean      237.032099\n",
      "std       179.778375\n",
      "min         4.000000\n",
      "25%       144.000000\n",
      "50%       220.000000\n",
      "75%       303.000000\n",
      "max      7467.000000\n",
      "Name: Original Text Special Chars, dtype: float64\n",
      "\n",
      "Preeti Text Special Chars:\n",
      "count    8536.000000\n",
      "mean       59.056350\n",
      "std        45.585538\n",
      "min         0.000000\n",
      "25%        35.000000\n",
      "50%        55.000000\n",
      "75%        75.000000\n",
      "max      2024.000000\n",
      "Name: Preeti Text Special Chars, dtype: float64\n",
      "\n",
      "Columns in the cleaned DataFrame:\n",
      "['File Name', 'Original Text', 'Preeti Text', 'Original Text Length', 'Preeti Text Length', 'Original Text Special Chars', 'Preeti Text Special Chars']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def clean_csv_file(input_csv, output_csv=None, \n",
    "                   original_text_column='unicode', \n",
    "                   preeti_text_column='preeti'):\n",
    "    \"\"\"\n",
    "    Clean CSV file by:\n",
    "    1. Removing rows with empty, null, or specific content\n",
    "    2. Removing leading tabs from original and preeti text columns\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_csv : str\n",
    "        Path to the input CSV file\n",
    "    output_csv : str, optional\n",
    "        Path to save the cleaned CSV file. \n",
    "        If None, will overwrite the input file\n",
    "    original_text_column : str, optional\n",
    "        Name of the original text column\n",
    "    preeti_text_column : str, optional\n",
    "        Name of the preeti text column\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        Cleaned DataFrame\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(input_csv, encoding='utf-8')\n",
    "        \n",
    "        # Print initial information\n",
    "        print(f\"Initial DataFrame Shape: {df.shape}\")\n",
    "        \n",
    "        # Function to check if a value is considered empty\n",
    "        def is_empty(value):\n",
    "            # Check for various types of \"emptiness\"\n",
    "            if pd.isna(value):  # Checks for NaN, None\n",
    "                return True\n",
    "            if isinstance(value, str):\n",
    "                # Check for empty string, string with only spaces, or specific empty markers\n",
    "                return (value.strip() == '' or \n",
    "                        value.lower() in ['', 'nan', 'null', '\"', \"'\", '\"\"'] or\n",
    "                        len(value.strip()) <= 2)\n",
    "            return False\n",
    "        \n",
    "        # Remove rows where specified column is empty\n",
    "        df_cleaned = df[~df[original_text_column].apply(is_empty)]\n",
    "        \n",
    "        # Function to remove leading tabs and whitespaces\n",
    "        def remove_leading_tabs_and_spaces(text):\n",
    "            if isinstance(text, str):\n",
    "                # Remove leading tabs and whitespaces\n",
    "                return text.lstrip('\\t ')\n",
    "            return text\n",
    "        \n",
    "        # Apply tab removal to both original and preeti text columns\n",
    "        df_cleaned[original_text_column] = df_cleaned[original_text_column].apply(remove_leading_tabs_and_spaces)\n",
    "        df_cleaned[preeti_text_column] = df_cleaned[preeti_text_column].apply(remove_leading_tabs_and_spaces)\n",
    "        \n",
    "        # Print information after cleaning\n",
    "        print(f\"Cleaned DataFrame Shape: {df_cleaned.shape}\")\n",
    "        print(f\"Rows Removed: {df.shape[0] - df_cleaned.shape[0]}\")\n",
    "        \n",
    "        # Save the cleaned DataFrame\n",
    "        if output_csv is None:\n",
    "            output_csv = input_csv  # Overwrite input file\n",
    "        \n",
    "        df_cleaned.to_csv(output_csv, index=False, encoding='utf-8')\n",
    "        print(f\"Cleaned CSV saved to: {output_csv}\")\n",
    "        \n",
    "        return df_cleaned\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "# Comprehensive analysis function\n",
    "def analyze_cleaned_data(df, \n",
    "                          original_text_column='Original Text', \n",
    "                          preeti_text_column='Preeti Text'):\n",
    "    \"\"\"\n",
    "    Perform detailed analysis on the cleaned DataFrame\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Data Cleaning Analysis ---\")\n",
    "    print(f\"Total Rows: {len(df)}\")\n",
    "    \n",
    "    # Original Text Analysis\n",
    "    print(\"\\n--- Original Text Analysis ---\")\n",
    "    print(\"Original Text Length Distribution:\")\n",
    "    df['Original Text Length'] = df[original_text_column].str.len()\n",
    "    print(df['Original Text Length'].describe())\n",
    "    \n",
    "    # Preeti Text Analysis\n",
    "    print(\"\\n--- Preeti Text Analysis ---\")\n",
    "    print(\"Preeti Text Length Distribution:\")\n",
    "    df['Preeti Text Length'] = df[preeti_text_column].str.len()\n",
    "    print(df['Preeti Text Length'].describe())\n",
    "    \n",
    "    # Unique value analysis\n",
    "    print(\"\\nUnique Values:\")\n",
    "    print(f\"Unique Original Texts: {df[original_text_column].nunique()}\")\n",
    "    print(f\"Unique Preeti Texts: {df[preeti_text_column].nunique()}\")\n",
    "    \n",
    "    # Check for remaining tabs\n",
    "    def contains_tab(text):\n",
    "        return '\\t' in str(text)\n",
    "    \n",
    "    # Check tabs in both columns\n",
    "    original_text_tabs = df[original_text_column].apply(contains_tab).sum()\n",
    "    preeti_text_tabs = df[preeti_text_column].apply(contains_tab).sum()\n",
    "    \n",
    "    print(\"\\n--- Tab Character Check ---\")\n",
    "    print(f\"Rows with Tab in Original Text: {original_text_tabs}\")\n",
    "    print(f\"Rows with Tab in Preeti Text: {preeti_text_tabs}\")\n",
    "\n",
    "# Optional extra analysis (uncomment if needed)\n",
    "def additional_text_analysis(df, \n",
    "                              original_text_column='Original Text', \n",
    "                              preeti_text_column='Preeti Text'):\n",
    "    \"\"\"\n",
    "    Perform additional detailed text analysis\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Advanced Text Analysis ---\")\n",
    "    \n",
    "    # Check for special characters\n",
    "    def count_special_chars(text):\n",
    "        return len(re.findall(r'[^a-zA-Z0-9\\s]', str(text)))\n",
    "    \n",
    "    df['Original Text Special Chars'] = df[original_text_column].apply(count_special_chars)\n",
    "    df['Preeti Text Special Chars'] = df[preeti_text_column].apply(count_special_chars)\n",
    "    \n",
    "    print(\"Special Characters Distribution:\")\n",
    "    print(\"Original Text Special Chars:\")\n",
    "    print(df['Original Text Special Chars'].describe())\n",
    "    print(\"\\nPreeti Text Special Chars:\")\n",
    "    print(df['Preeti Text Special Chars'].describe())\n",
    "\n",
    "# Example usage in Jupyter Lab\n",
    "# Specify your input and output file paths\n",
    "input_file = 'infotech.csv'\n",
    "output_file = 'infotech_cleaned.csv'\n",
    "\n",
    "# Clean the CSV file\n",
    "cleaned_df = clean_csv_file(input_file, output_file)\n",
    "\n",
    "# Perform analysis if cleaning is successful\n",
    "if cleaned_df is not None:\n",
    "    # Display first few rows\n",
    "    print(\"\\nFirst few rows of cleaned DataFrame:\")\n",
    "    display(cleaned_df.head())\n",
    "    \n",
    "    # Perform comprehensive analysis\n",
    "    analyze_cleaned_data(cleaned_df)\n",
    "    \n",
    "    # Optional: Run additional text analysis\n",
    "    additional_text_analysis(cleaned_df)\n",
    "\n",
    "# Optional: Print column names to verify\n",
    "print(\"\\nColumns in the cleaned DataFrame:\")\n",
    "print(cleaned_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb64c6dc-a753-44fb-a683-d4d90325dade",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged 2 CSV files into 'merged_output.csv' without 'File Name' column.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Folder path where your CSVs are located\n",
    "folder_path = 'rawdata/'\n",
    "\n",
    "# Get all CSV file paths\n",
    "csv_files = glob.glob(os.path.join(folder_path, '*.csv'))\n",
    "\n",
    "# Read and concatenate all CSVs, dropping the \"File Name\" column if it exists\n",
    "merged_df = pd.concat(\n",
    "    [pd.read_csv(f).drop(columns=[\"File Name\"], errors='ignore') for f in csv_files],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "# Save to a new merged CSV file\n",
    "merged_df.to_csv('unicode_preeti_dataset.csv', index=False)\n",
    "\n",
    "print(f\"Merged {len(csv_files)} CSV files into 'merged_output.csv' without 'File Name' column.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7411162e-2c59-4e6e-8783-859a20d5f71e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
