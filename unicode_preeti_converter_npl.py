# -*- coding: utf-8 -*-
"""unicode_preeti_converter_npl.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/108tXaMPwR-uVApgPdYmBXng0yv3pxMdA
"""

!pip install torch transformers pandas matplotlib scikit-learn --force

!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

!pip uninstall torch torchvision torchaudio -y
!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

import os
import logging
import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, get_linear_schedule_with_warmup
from torch.optim import AdamW
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from tqdm import tqdm

# Speed up
torch.backends.cudnn.benchmark = True

# Disable tokenizers parallelism warning
os.environ['TOKENIZERS_PARALLELISM'] = 'false'

# Logging setup
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s: %(message)s')
logger = logging.getLogger(__name__)

# Device setup
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
logger.info(f"Using device: {device}")
if torch.cuda.is_available():
    logger.info(f"GPU: {torch.cuda.get_device_name(0)}")

# Dataset Class
class UnicodePreetiFontDataset(Dataset):
    def __init__(self, unicode_texts, preeti_texts, tokenizer, max_length=128):
        self.unicode_texts = unicode_texts
        self.preeti_texts = preeti_texts
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.unicode_texts)

    def __getitem__(self, idx):
        unicode_text = str(self.unicode_texts[idx])
        preeti_text = str(self.preeti_texts[idx])

        encoding = self.tokenizer(
            unicode_text,
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt'
        )

        target_encoding = self.tokenizer(
            preeti_text,
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt'
        )

        return {
            'input_ids': encoding['input_ids'].squeeze(),
            'attention_mask': encoding['attention_mask'].squeeze(),
            'labels': target_encoding['input_ids'].squeeze()
        }

# Accuracy calculation
def calculate_accuracy(preds, labels):
    preds = preds.view(-1)
    labels = labels.view(-1)
    mask = labels != -100
    correct = (preds[mask] == labels[mask]).sum().item()
    total = mask.sum().item()
    return correct / total if total > 0 else 0

# Plotting function
def plot_metrics(epochs, train_losses, val_losses, train_accuracies, val_accuracies):
    plt.figure(figsize=(12, 5))

    plt.subplot(1, 2, 1)
    plt.plot(epochs, train_losses, label='Train Loss', color='blue')
    plt.plot(epochs, val_losses, label='Validation Loss', color='red')
    plt.title('Loss over Epochs')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()

    plt.subplot(1, 2, 2)
    plt.plot(epochs, train_accuracies, label='Train Accuracy', color='green')
    plt.plot(epochs, val_accuracies, label='Validation Accuracy', color='orange')
    plt.title('Accuracy over Epochs')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()

    plt.tight_layout()
    plt.savefig('training_metrics_plot.png')
    plt.close()

# Load CSV dataset
def load_dataset(path, sample_fraction=1.0):
    df = pd.read_csv(path, encoding='utf-8')
    df = df.sample(frac=sample_fraction, random_state=42)

    if 'unicode' not in df.columns or 'preeti' not in df.columns:
        raise ValueError("CSV must contain 'unicode' and 'preeti' columns")

    return df['unicode'].tolist(), df['preeti'].tolist()

# Model Training
def train_model(unicode_texts, preeti_texts, model_name='facebook/nllb-200-distilled-600M', epochs=5):
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)

    train_texts, val_texts, train_labels, val_labels = train_test_split(unicode_texts, preeti_texts, test_size=0.2, random_state=42)

    train_dataset = UnicodePreetiFontDataset(train_texts, train_labels, tokenizer)
    val_dataset = UnicodePreetiFontDataset(val_texts, val_labels, tokenizer)

    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=8)

    optimizer = AdamW(model.parameters(), lr=3e-5)
    total_steps = len(train_loader) * epochs
    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=int(0.1 * total_steps), num_training_steps=total_steps)

    train_losses, val_losses = [], []
    train_accuracies, val_accuracies = [], []
    epoch_list = []

    for epoch in range(epochs):
        model.train()
        total_loss = 0
        total_acc = 0

        for batch in tqdm(train_loader, desc=f"Epoch {epoch+1}/{epochs} - Training"):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
            loss = outputs.loss
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            scheduler.step()
            optimizer.zero_grad()

            total_loss += loss.item()

            preds = outputs.logits.argmax(dim=-1)
            acc = calculate_accuracy(preds, labels)
            total_acc += acc

        avg_train_loss = total_loss / len(train_loader)
        avg_train_acc = total_acc / len(train_loader)

        # Validation
        model.eval()
        val_loss = 0
        val_acc = 0

        with torch.no_grad():
            for batch in tqdm(val_loader, desc=f"Epoch {epoch+1}/{epochs} - Validation"):
                input_ids = batch['input_ids'].to(device)
                attention_mask = batch['attention_mask'].to(device)
                labels = batch['labels'].to(device)

                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
                val_loss += outputs.loss.item()

                preds = outputs.logits.argmax(dim=-1)
                val_acc += calculate_accuracy(preds, labels)

        avg_val_loss = val_loss / len(val_loader)
        avg_val_acc = val_acc / len(val_loader)

        logger.info(f"Epoch {epoch+1} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Train Acc: {avg_train_acc:.4f} | Val Acc: {avg_val_acc:.4f}")

        epoch_list.append(epoch+1)
        train_losses.append(avg_train_loss)
        val_losses.append(avg_val_loss)
        train_accuracies.append(avg_train_acc)
        val_accuracies.append(avg_val_acc)

    plot_metrics(epoch_list, train_losses, val_losses, train_accuracies, val_accuracies)

    model.save_pretrained("unicode_to_preeti_model")
    tokenizer.save_pretrained("unicode_to_preeti_model")

    return model, tokenizer

# Inference
def inference(text, model, tokenizer):
    inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=128).to(device)
    outputs = model.generate(inputs.input_ids, max_length=128, num_beams=4)
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# Main function
def main():
    unicode_texts, preeti_texts = load_dataset("/content/drive/MyDrive/thesis/phrase_dataset_unicode_to_preeti.csv", sample_fraction=1.0)

    model, tokenizer = train_model(unicode_texts, preeti_texts, epochs=5, model_name = "Helsinki-NLP/opus-mt-en-ROMANCE")

    print("\n--- Inference Samples ---")
    for i in range(3):
        sample = unicode_texts[i]
        result = inference(sample, model, tokenizer)
        print(f"Unicode: {sample}")
        print(f"Preeti : {result}\n")

if __name__ == "__main__":
    main()

import os
import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import logging
import pandas as pd

# Logging setup
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s: %(message)s')
logger = logging.getLogger(__name__)

# Device setup
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
logger.info(f"Using device: {device}")

class UnicodePreetiFontConverter:
    def __init__(self, model_path="unicode_to_preeti_model"):
        """
        Initialize the converter with pre-trained model

        Args:
            model_path (str): Path to saved model directory
        """
        try:
            # Load tokenizer and model
            self.tokenizer = AutoTokenizer.from_pretrained(model_path)
            self.model = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(device)
            self.model.eval()  # Set to evaluation mode

            logger.info("Model loaded successfully!")
        except Exception as e:
            logger.error(f"Model loading error: {e}")
            raise

    def convert_single(self, unicode_text, max_length=128):
        """
        Convert single Unicode text to Preeti

        Args:
            unicode_text (str): Input Unicode text
            max_length (int): Maximum length for tokenization

        Returns:
            str: Converted Preeti text
        """
        try:
            # Prepare input
            inputs = self.tokenizer(
                unicode_text,
                return_tensors='pt',
                truncation=True,
                max_length=max_length
            ).to(device)

            # Generate translation
            outputs = self.model.generate(
                inputs.input_ids,
                max_length=max_length,
                num_beams=4,  # Beam search for better results
                early_stopping=True
            )

            # Decode output
            preeti_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            return preeti_text

        except Exception as e:
            logger.error(f"Conversion error for text '{unicode_text}': {e}")
            return None

    def batch_convert(self, unicode_texts, max_length=128):
        """
        Convert multiple Unicode texts to Preeti

        Args:
            unicode_texts (list): List of Unicode texts
            max_length (int): Maximum length for tokenization

        Returns:
            list: List of converted Preeti texts
        """
        return [self.convert_single(text, max_length) for text in unicode_texts]

    def convert_from_csv(self, csv_path, unicode_column='unicode', batch_size=32):
        """
        Convert Unicode texts from CSV file

        Args:
            csv_path (str): Path to CSV file
            unicode_column (str): Name of Unicode column
            batch_size (int): Number of texts to process in each batch

        Returns:
            pd.DataFrame: DataFrame with original and converted texts
        """
        try:
            # Read CSV
            df = pd.read_csv(csv_path, encoding='utf-8')

            if unicode_column not in df.columns:
                raise ValueError(f"Column '{unicode_column}' not found in CSV")

            # Convert texts in batches
            converted_texts = []
            for i in range(0, len(df), batch_size):
                batch = df[unicode_column][i:i+batch_size].tolist()
                converted_batch = self.batch_convert(batch)
                converted_texts.extend(converted_batch)

            # Add converted column
            df['preeti_converted'] = converted_texts

            return df

        except Exception as e:
            logger.error(f"CSV conversion error: {e}")
            return None

def main():
    # Initialize converter
    converter = UnicodePreetiFontConverter()

    # Single text conversion examples
    print("\n--- Single Text Conversion ---")
    test_texts = ["क", "का", "कि", "की"]
    for text in test_texts:
        preeti_text = converter.convert_single(text)
        print(f"Unicode: {text} | Preeti: {preeti_text}")

    # CSV conversion example
    print("\n--- CSV Batch Conversion ---")
    csv_path = 'sample_unicode_texts.csv'

    # Ensure sample CSV exists
    if not os.path.exists(csv_path):
        sample_data = {
            'unicode': ['क', 'का', 'कि', 'की', 'कु', 'के', 'अ', 'इ', 'उ'],
            'description': ['Sample Nepali Character', 'ka', 'ki', 'kI', 'ku', 'ke', 'a', 'i', 'u']
        }
        pd.DataFrame(sample_data).to_csv(csv_path, index=False)
        print(f"Created sample CSV: {csv_path}")

    # Convert CSV
    converted_df = converter.convert_from_csv(csv_path)
    if converted_df is not None:
        print("\nConverted DataFrame:")
        print(converted_df)

        # Optional: Save converted results
        converted_df.to_csv('converted_unicode_to_preeti.csv', index=False)
        print("Saved converted results to 'converted_unicode_to_preeti.csv'")

from IPython.display import display, HTML

def display_preeti_text(text):
    html = f"""
    <style>
    @font-face {{
        font-family: 'Preeti';
        src: url('Preeti.ttf') format('truetype');
    }}
    .preeti-text {{
        font-family: 'Preeti';
        font-size: 20px;
    }}
    </style>
    preeti font:<div class="preeti-text"> {text}</div></br>
    """
    display(HTML(html))

def advanced_text_processing_demo():
    """
    Advanced text processing demonstration
    """
    converter = UnicodePreetiFontConverter()

    # Complex text processing scenarios
    complex_texts = [
        "नमस्ते",  # Greeting
        "डाटा विज्ञान",  # Data Science
        "कम्प्युटर",  # Computer
        "मेराे नाम सुमन हाे । what is yours?",
        "मेराे नाम पुजन हाे ।  तिमी के गछाै ",
        "गाराे छ केटा हाे । स्यालरी कहिले अाउने हाे "
    ]

    print("\n--- Advanced Text Processing ---")
    for text in complex_texts:
        preeti_text = converter.convert_single(text)
        print(f"Unicode: {text}")
        print(f"Preeti : {preeti_text}\n")
        predicted_text = display_preeti_text(preeti_text)

if __name__ == "__main__":
    main()
    advanced_text_processing_demo()